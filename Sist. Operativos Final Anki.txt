#separator:tab
#html:true
#deck column:1
#tags column:4
Sist. Operativos Final	What are the ways in which RAM (physical memory) is managed by the OS?	Memory management schemes:<br><br><i>- Contiguous</i><br><ul><li><u><b>Fixed-partition scheme:</b></u> fixed-size blocks. Produces internal fragmentation. Blocks don't necessarily have to be the same size.</li><li><u><b>Variable-partition scheme:</b></u> dynamically-sized blocks. Different allocation (at load time) strategies designed to reduce external fragmentation include: first-fit, best-fit, worst-fit.&nbsp;</li></ul><i>- Non-contiguous:</i>&nbsp;<br><ul><li><u><b>Paging:</b></u> Processes are divided into fized-size pages and RAM is divided into frames of the same size that are allocated to said pages. External fragmentation is eliminated.&nbsp;</li><li><b><u>Segmentation:</u></b> Divides processes into logical segments of varying sizes (code, heap, stack). Vulnerable to external fragmentation.&nbsp;</li><li><b><u>Paged segmentation:</u></b> Combines both schemas through the use of segments divided into pages. Vulnerable to both external and internal fragmentation.</li></ul>	memory-admin
Sist. Operativos Final	What is swapping? What is it's purpose? What criteria is used to differentiate between variants like roll in, roll out?	"Swapping moves processes between RAM and secondary storage (disk-based storage) to free memory and increase multiprogramming capacity; the criteria used to differentiate between swapping variants is what exactly is loaded onto main memory:<br><ul><li>A whole process (full-process swapping)</li><li>Needed pages/segments (partial); includes segment swapping and paged segmentation swapping (subtypes)<br></li></ul><div><br><img alt=""swapping"" src=""swapping.webp""><br></div>"	memory-admin
Sist. Operativos Final	What is address binding? How do we differentiate between the 3 types of address binding?	"Address binding maps logical to physical addresses; it is classified as compile-time, load-time, or execution-time based on when the binding occurs.<br><br><img alt=""source_code_"" src=""source_code_.webp"">"	memory-admin
Sist. Operativos Final	What stages does a program go through to go from source code to running on a computer?	"Compilation: Source code goes from human-readable variable and function names to either fixed absolute addresses or offsets/relocatable code. Depends on if we know the physical base address or not/compile-time binding is being employed or not.&nbsp;<br><br>Loading: Brings the compiled program to main memory. If the base address was resolved pre-compilation, it copies the absolute addresses into RAM. If it wasn't, the loader first resolves the offsets by adding the base address to produce the final physical addresses, UNLESS the OS uses execution-time binding.<br><br>Execution: The CPU fetches and runs the program instructions loaded onto the RAM. Unresolved addresses are handled dynamically if execution-time binding is being used with the help of the MMU.&nbsp;<br><br><img alt=""Address Binding and its Types - GeeksforGeeks"" src=""frame_27.webp"">"	memory-admin
Sist. Operativos Final	What does the MMU do?	The MMU translates logical (virtual) addresses to physical addresses and enforces memory protection by checking bounds and permission info. before each memory access. Think of a sheriff	memory-admin
Sist. Operativos Final	Difference in physical address calculation by the MMU based on memory management scheme	"- Paging:&nbsp;<br>PA = frame base + page offset (frame base corresponding to the frame belonging to a page according to the page table; position of the address within it's page)<br><br>- Segmentation:<br>PA = segment base + segment offset<br><br>- Paged segmentation combines the two above steps<br><br>- Contiguous memory schemes:&nbsp;<br>PA = base address + logical address<br><br><img alt=""What is Memory Management Unit (MMU)? | Engineer's Portal"" src=""Memory-Management-F10.png""><br><br><img src=""paging-shenanigans.png"">"	memory-admin
Sist. Operativos Final	Static vs dynamic memory allocation	Static memory allocation assigns memory at compile/load time, while dynamic allocation assigns memory at runtime.	memory-admin
Sist. Operativos Final	What are shared libraries? What is a stub?	Shared libraries contain reusable code loaded by multiple programs; a stub is a placeholder in a program for a procedure in a shared library or remote call.	memory-admin
Sist. Operativos Final	What is a TLB and what does it do?	A Translation Lookaside Buffer is a small cache (limited-size, high-speed memory that stores frequently used data) that stores recent page-to-frame translations to speed up address translation.<br><br>It is quick thanks to its smaller size compared to a process' page table and proximity to the CPU when compared to main memory.	memory-admin
Sist. Operativos Final	Describe the structure of a virtual address generated by the CPU for translation by the MMU. How do we compute the amount of necessary bits to represent a physical address in a given computer system?&nbsp;	Paging:&nbsp;<br>| Page Number (p) | Page Offset or Displacement (d) |<br><br>p = log2(number of pages in virtual address space)<br>d = log2(page size in bytes)<br><br>PA bits:<br>PA bits = log2(number of frames in RAM * frame/page size in Bytes)	memory-admin
Sist. Operativos Final	When does a segfault occur?	A segfault occurs when a process tries to access a physical address outside of it's bounds. Sequence of actions:<br><ul><li>The CPU generates a virtual address</li><li>The MMU checks that the virtual address/offset is smaller than the limit register of the given process</li><li>If true, it computes the physical address</li><li>Otherwise, a segfault occurs</li></ul>	memory-admin
Sist. Operativos Final	1KB =&gt; ? Bytes	1024 bytes	memory-admin
Sist. Operativos Final	How can I determine the better schema in variable-size contiguous memory management?	Using the largest remaining contiguous block is a practical way to compare allocation efficiency when all current processes fit in two or three given schemas	memory-admin
Sist. Operativos Final	Info. associated to a page table entry	"<img alt=""112"" src=""112.webp""><br><br>Frame number: frame associated to the given page (identified by the index)<br>Present/absent: Is it in main memory or not?<br>Protection Bits: Define the allowed operations on a page (Read, Write and Execute)<br>Referenced bit: Bit that indicates if a page has recently been accessed or not. It is periodically cleared by the OS (for example, every 100ms) to monitor recent usage<br>Caching: If it is set to 1 the page can be stored in cache for faster access. If it is 0, the CPU will always have to access the main memory to fetch the page<br>Dirty/Modified: The dirty/modified bit indicates if a page has been modified since it was loaded onto RAM by the loader and needs to be written to the disk"	memory-admin
Sist. Operativos Final	What is a pagefault and how does the O.S. deal with one?	1. A pagefault happens when the CPU generates a virtual address that is associated to a page that has it's present/valid bit in it's corresponding entry in the page table set to 0. This is verified by the MMU<br><br>2. A free frame is found or a victim page is selected to be evicted using a page replacement algorithm<br><br>3. When this happens, the OS takes control and identifies the process that triggered the pagefault and asks the disk to read the required page into the selected frame<br><br>4. The page is loaded from disk (swapped in)<br><br>5. The page table is updated and the CPU tries once more to access the page. Now that the page is in RAM, it is successful	memory-admin
Sist. Operativos Final	What is Belady's anomaly?	"Belady's anomaly is a term referring to the fact that as the frames available in RAM increases, the chances of a page fault occuring in SOME page replacement algorithms increases.<br><br>For example, in the FIFO page replacement algorithm:<br><br><img src=""paste-349d06f0db095894d2658a416b28ad3ff69270b8.jpg"">"	memory-admin
Sist. Operativos Final	What is the target of all page replacement algorithms?	The target is to reduce the number of page faults that occur when the CPU tries to access a given page&nbsp;	memory-admin
Sist. Operativos Final	What page replacement algorithms avoid Belady's anomaly and which ones don't? Why?	Algorithms that avoid the anomaly:<br><br>- Optimal: Theoretical benchmark used for comparison where an OS removes the page that'll be used farthest in the future. Produces the least page faults.<br><br>- LRU: Removes the page that was least recently used<br><br>They avoid the anomaly because they are stack-based algorithms that, when working with more frames, don't inadvertently evict pages that were being kept with fewer frames.&nbsp;<br><br>Algorithms that don't avoid it include FIFO, Most Recently Used and Random (evicts an arbitrary page)	memory-admin
Sist. Operativos Final	What is thrashing? (hiperpaginación)	It's a term that references the phenomenon where the system spends most of it's time handling page faults and swapping pages in and out of disk instead of executing processes	memory-admin
Sist. Operativos Final	How does a system eliminate thrashing?&nbsp;	Thrashing can happen when there's a high degree of multiprogramming or a lack of frames&nbsp;<br><br>Measures used to deal with thrashing:&nbsp;<br><br>Working set model: Keeps whole process' working set in RAM (according to estimations on behalf of the OS), making it so that processes that are too big to execute their working set on available frames wait until space is sufficient<br><br>Page Fault Frequency: Adjusts frames allocated to processes dynamically based on page fault rate<br><br>Load control: limits number of active processes<br><br>Local page replacement: Makes it so that pages of a process can only be replaced by pages of the same process	memory-admin
Sist. Operativos Final	How does a system detect thrashing?	Indicators used by operating systems to detect thrashing:<br>1. High page fault rate<br>2. Low CPU utilization with high disk I/O activity<br>3. Active processes with working sets larger than available RAM	memory-admin
Sist. Operativos Final	What is equal frame allocation?	"When the system allocates an equal amount of frames to each process regardless of the amount of entries in it's page table. It's not a ""fair"" system in terms of performance&nbsp;&nbsp;"	memory-admin
Sist. Operativos Final	"""Proportional"" frame allocation"	Frames are distributed among processes proportional to the size of each process. For example,&nbsp;<br><br>Total available frames: 80<br><br>Process A:<br>Pages: 10<br>Allocated frames: 80*(10/40) = 20<br><br>Process B:&nbsp;<br>Pages: 30<br>Allocated frames: 80*(30/40) = 60	memory-admin
Sist. Operativos Final	"What does the term ""global page replacement"" reference?"	Global frame replacement is a policy where a page faulting process may replace any page in RAM according to the page replacement algorithm in use, not only pages belonging to itself. This increases flexibility and overall memory utilization, but allows processes to take frames from others, potentially increasing their page fault rate and causing unfairness.	memory-admin
Sist. Operativos Final	Name the fundamental property of a stack algorithm in page replacement	<div>A stack algorithm is a page replacement algorithm with this property:</div> <div>If a page would be in memory with n frames, it will also be in memory with n+1 frames.</div>	memory-admin
Sist. Operativos Final	When is FIFO &gt; LRU true?	FIFO can prove to be a superior PRA in comparison to LRU thanks to it's significantly lower overhead, lesser amount of reads from memory and lesser associated metadata. Not to mention that, in the case of a page reference string that is cyclical in nature (i.e. 1 2 3 4 1 2 5 1 2 3 4 5, assuming 3 frames), it can even produce fewer PFs than LRU<div><br></div><div>Systems where the CPU can often request access to pages in a cyclical way include, for example, embedded plane systems</div>	memory-admin
Sist. Operativos Final	Virtual vs. Physical addresses	Virtual addresses are addresses that indicate a location within the virtual space of a process, that the CPU uses as an interface to interact with RAM with the help of the MMU<br><br>Physical addresses refer to locations in actual physical main memory and are obtained by different OS components depending on the address binding type employed:<br><ul><li>Compile-time binding: the compiler (happens if a base physical address is known a priori)</li><li>Load-time binding: the physical addresses are resolved by the loader</li><li>Execution-time binding: the MMU resolves the addresses using a given process' page table</li></ul>NOTE: The loader doesn't use page tables, not even if load-time binding is in use. This is because a process' page table is generated strictly at execution time alongside other process management data structures such as the PCB<br>	memory-admin
Sist. Operativos Final	When does a minor page fault occur?	A soft (minor) page fault occurs when the requested page is not currently mapped in the process’s virtual address space, but the page already exists somewhere in RAM, so the OS can resolve it without disk I/O. This can happen in two main scenarios:<br><br>- The page is in page cache (portion of RAM allocated to recently referenced pages)<br>- The page is currently in a different process's physical memory space&nbsp;	memory-admin
Sist. Operativos Final	What is copy-on-write?	Copy-on-write is a memory optimization technique used by operating systems to delay copying a memory page until it is actually modified. It allows multiple processes to share the same physical page in RAM until one of them writes to it.	memory-admin
Sist. Operativos Final	When will a write-back not occur in the case of a page fault?	A page fault caused by a read request occurs when the process tries to access a code or file page not currently in RAM.<br> The OS loads the page into a free frame or replaces a clean (unmodified) page if needed. Only a read is required; no write-back is necessary	memory-admin
Sist. Operativos Final	"What is a ""constrained buffer""?"	A constrained buffer refers to a space in the OS' address space that a producing process writes data to and that a consuming process reads from<br><br><i>A use case could be one like the following:</i><br>A producing process writing to a queue of integers that are read and removed from the queue by a different consuming process that stores them in a text file	process-synch
Sist. Operativos Final	"What is the ""producer-consumer problem"" and what is it's solution?"	"The problem lies in ensuring that there is space in the bounded buffer before producing and that there are items to be read before consuming. Synchronization between processes is necessary to achieve this and prevent race conditions/data corruption<br><br>The solution to the producer-consumer problem is found in the operations surrounding the critical sections in which the buffer is accessed by the participating processes: P (try to decrease/wait) and V (increment/signal)<br><br>Alternatively, busy waiting can be used<br><br><img alt=""Semaphores, Nachos"" src=""oct02img1.gif"">"	process-synch
Sist. Operativos Final	What do wait() and signal() do?	They are operations that decrement (or try to) and increment a given semaphore respectively.&nbsp;<br>When implemented in the context of bounded buffers, the semaphore value corresponds to the number of available slots at a given time<br><br>P and V must be executed atomically: they cannot be interrupted once they begin execution and they cannot execute simultaneously. They modify the value of only one semaphore at a time	process-synch
Sist. Operativos Final	Semaphore types and their typical use cases	<b>Counting semaphores:</b> control access to a resource with multiple instances, like a bounded buffer. Can take any non-negative int value and represents the number of available resources<br><br><b>Binary semaphore:</b> used for mutual exclusion and can take only the values of 0 and 1. It ensures that only one process enters the critical section at a time	process-synch
Sist. Operativos Final	What is busy waiting?	"Busy waiting is a less optimal solution to the producer-consumer problem. It works through the continuous check of a given condition, such as a full buffer or empty buffer in a loop before proceeding into it's critical section.&nbsp;<br><br>It is less optimal because it wastes CPU cycles by staying in the running state rather than sleeping, even if it isn't doing anything but checking a condition.<br><br><img alt=""Mutual Exclusion with Busy Waiting (Software approach)"" src=""sAuUaQxYMjJQa0VeAsgaaKRFbc7BLGTRd4cxas3Rq3oY9LTfjqwV4pa4gm5isVaCmWfKoJLP-HAARGtlI9BltbrBYYiGRCJAnlv-aJzBulZ2Uc9uu-s.png"">"	process-synch
Sist. Operativos Final	How does semaphore usage spare CPU resources?	"Semaphore usage spares CPU resources through the following mechanism, assuming a Linux based system and no busy waiting:<br><br>When a process performs a wait() operation and the semaphore value is 0, the process is put to sleep by the OS and placed in the semaphore's waiting queue<br><br>When a different process performs a signal() operation on that same semaphore and releases the semaphore, the waiting process is then promptly ""woken up"" and once more put into the running state<br><br>By implementing syscalls and performing privileged operations, we evade busy waiting through temporary process suspension&nbsp;"	process-synch
Sist. Operativos Final	Producer-consumer problem vs. critical section problem	The critical section problem is a generic term that references the risk of race conditions in the case of two or more processes concurrently accessing shared resources. The solution is any mechanism that allows only one process to access the shared resource, ensures no process waits indefinitely and that allows progress<br><br>The producer-consumer problem is a specific instance of the critical section problem that applies specifically to shared buffers	process-synch
Sist. Operativos Final	What 3 requirements determine if a solution to the critical section problem is valid or not?	Bounded waiting: it's purpose lies in making sure that a process only waits so long to execute it's critical section. It makes it so that no process is starved. For example: semaphores with waiting queues, a lock with an associated FIFO queue&nbsp;<br><br>Mutual exclusion: Only one process can enter it's critical section at a time<br><br>Progress: If no process is in the critical section and some wish to enter, the choice of the next process must not be postponed indefinitely, and only processes that are trying to enter can participate in that decision.	process-synch
Sist. Operativos Final	Solutions that exist for the consumer-producer problem:	<ul><li>Peterson's algorithm (implements bounded waiting)</li><li>Semaphores (implemented with the use of wait() and signal())</li><li>Interrupt disabling on systems with a single CPU core (not recommended due to the fact it hurts responsiveness and blocks preemption)</li></ul>	process-synch
Sist. Operativos Final	Preemption vs interruption	Preemption: The forceful seizure of CPU resources from one process to allocate them to another one as a result of scheduling policies (for example: round robin, process requests I/O)<br><br>Interruption: A signal (either from hardware or software) that halts the CPU's current activities to give attention to something more urgent<br><br>Relation: interrupts often cause preemption. For example, hardware interrupt -&gt; CPU jumps to interrupt handler in kernel mode -&gt; handler finishes -&gt; scheduler makes decision -&gt; CPU either resumes the same process or preempts the process	scheduling
Sist. Operativos Final	What is Peterson's algorithm and how does it work?	"It is a solution to the critical section problem (where only one process at a time can access a shared resource) specifically for two processes.&nbsp;<br><br>It uses shared variables and busy waiting<br><br>Shared variables: an array of two flags and a 'turn' variable<br><br><img src=""paste-983725e7fbedc7ccc16ca228625bb22e8952b061.jpg"">"	process-synch
