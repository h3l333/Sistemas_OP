#separator:tab
#html:true
#deck column:1
#tags column:4
Sist. Operativos Final	What are the ways in which RAM (physical memory) is managed by the OS?	Memory management schemes:<br><br><i>- Contiguous</i><br><ul><li><u><b>Fixed-partition scheme:</b></u> fixed-size blocks. Produces internal fragmentation. Blocks don't necessarily have to be the same size.</li><li><u><b>Variable-partition scheme:</b></u> dynamically-sized blocks. Different allocation (at load time) strategies designed to reduce external fragmentation include: first-fit, best-fit, worst-fit.&nbsp;</li></ul><i>- Non-contiguous:</i>&nbsp;<br><ul><li><u><b>Paging:</b></u> Processes are divided into fized-size pages and RAM is divided into frames of the same size that are allocated to said pages. External fragmentation is eliminated.&nbsp;</li><li><b><u>Segmentation:</u></b> Divides processes into logical segments of varying sizes (code, heap, stack). Vulnerable to external fragmentation.&nbsp;</li><li><b><u>Paged segmentation:</u></b> Combines both schemas through the use of segments divided into pages. Vulnerable to both external and internal fragmentation.</li></ul>	memory-admin
Sist. Operativos Final	What is swapping? What is it's purpose? What criteria is used to differentiate between swapping variants?	"Swapping moves processes between RAM and secondary storage (disk-based storage) to free memory and increase multiprogramming capacity- swapping pages/segments in and out does not qualify as ""swapping"", rather that schema refers to paging/segmentation/paged segmentation, memory management schemas<br><div>NOTE: Roll-in/roll-out refers SPECIFICALLY to one form of swapping where processes are swapped in and out of main memory based on their priority. If a high-priority process enters the ready queue, a lower-priority process might be swapped out to allocate more resources to the higher-priority process</div><div><br><img alt=""swapping"" src=""swapping.webp""><br></div>"	memory-admin
Sist. Operativos Final	What is address binding? How do we differentiate between the 3 types of address binding?	"Address binding maps logical to physical addresses; it is classified as compile-time, load-time, or execution-time based on when the binding occurs.<br><br><img alt=""source_code_"" src=""source_code_.webp"">"	memory-admin
Sist. Operativos Final	What stages does a program go through to go from source code to running on a computer? Associate physical address binding to the process	"Compilation: Source code goes from human-readable variable and function names to either fixed absolute addresses or offsets/relocatable code. Depends on if we know the physical base address beforehand or not/compile-time binding is being employed or not.&nbsp;<br><br>Loading: Brings the compiled program to main memory. If the base address was resolved pre-compilation, it copies the absolute addresses into RAM. If it wasn't, the loader first resolves the offsets by adding the base address to produce the final physical addresses, UNLESS the OS uses execution-time binding.<br><br>Execution: The CPU fetches and runs the program instructions loaded onto the RAM. Unresolved addresses are handled dynamically if execution-time binding is being used with the help of the MMU.&nbsp;<br><br><img alt=""Address Binding and its Types - GeeksforGeeks"" src=""frame_27.webp"">"	os-structure
Sist. Operativos Final	What does the MMU do?	The MMU translates logical (virtual) addresses to physical addresses and enforces memory protection by checking bounds and permission info. before each memory access. Think of a sheriff	memory-admin
Sist. Operativos Final	Difference in physical address calculation by the MMU based on memory management scheme	"- Paging:&nbsp;<br>PA = frame base + page offset (frame base corresponding to the frame belonging to a page according to the page table; position of the address within it's page)<br><br>- Segmentation:<br>PA = segment base + segment offset<br><br>- Paged segmentation combines the two above steps<br><br>- Contiguous memory schemes:&nbsp;<br>PA = base address + logical address<br><br><img alt=""What is Memory Management Unit (MMU)? | Engineer's Portal"" src=""Memory-Management-F10.png""><br><br><img src=""paging-shenanigans.png"">"	memory-admin
Sist. Operativos Final	Static vs dynamic memory allocation	Static memory allocation assigns memory at compile/load time, while dynamic allocation assigns memory at runtime.	memory-admin
Sist. Operativos Final	What are shared libraries? What is a stub?	Shared libraries contain reusable code loaded by multiple programs; a stub is a placeholder in a program for a procedure in a shared library or remote call.	memory-admin
Sist. Operativos Final	What is a TLB and what does it do?	A Translation Lookaside Buffer is a small cache (limited-size, high-speed memory that stores frequently used data) that stores recent page-to-frame translations to speed up address translation.<br><br>It is quick thanks to its smaller size compared to a process' page table and proximity to the CPU when compared to main memory.	memory-admin
Sist. Operativos Final	Describe the structure of a virtual address generated by the CPU for translation by the MMU. How do we compute the amount of necessary bits to represent a physical address in a given computer system?&nbsp;	Paging:&nbsp;<br>| Page Number (p) | Page Offset or Displacement (d) |<br><br>p = log2(number of pages in virtual address space)<br>d = log2(page size in bytes)<br><br>PA bits:<br>PA bits = log2(number of frames in RAM * frame/page size in Bytes)	memory-admin
Sist. Operativos Final	When does a segfault occur?	A segfault occurs when a process tries to access a physical address outside of it's bounds. Sequence of actions:<br><ul><li>The CPU generates a virtual address</li><li>The MMU checks that the virtual address/offset is smaller than the limit register of the given process</li><li>If true, it computes the physical address</li><li>Otherwise, a segfault occurs</li></ul>	memory-admin
Sist. Operativos Final	1KB =&gt; ? Bytes	1024 bytes	memory-admin
Sist. Operativos Final	How can I determine the better schema in variable-size contiguous memory management?	Using the largest remaining contiguous block is a practical way to compare allocation efficiency when all current processes fit in two or three given schemas	memory-admin
Sist. Operativos Final	Info. associated to a page table entry (6 total 'fields')	"<img alt=""112"" src=""112.webp""><br><br>Frame number: frame associated to the given page (identified by the index)<br>Present/absent: Is it in main memory or not?<br>Protection Bits: Define the allowed operations on a page (Read, Write and Execute)<br>Referenced bit: Bit that indicates if a page has recently been accessed or not. It is periodically cleared by the OS (for example, every 100ms) to monitor recent usage<br>Caching: If it is set to 1 the page can be stored in cache for faster access. If it is 0, the CPU will always have to access the main memory to fetch the page<br>Dirty/Modified: The dirty/modified bit indicates if a page has been modified since it was loaded onto RAM by the loader and needs to be written to the disk"	memory-admin
Sist. Operativos Final	What is a pagefault and how does the O.S. deal with one?	"1. A pagefault happens when the CPU generates a virtual address that is associated to a page that has it's present/valid bit in it's corresponding entry in the page table set to 0. This is verified by the MMU<br><br>2. A free frame is found or a victim page is selected to be evicted using a page replacement algorithm<br><br>3. When this happens, the OS takes control and identifies the process that triggered the pagefault through a trap and asks the disk to read the required page into the selected frame<br><br>4. The page is loaded from disk (swapped in)<br><br>5. The page table is updated and the CPU tries once more to access the page. Now that the page is in RAM, it is successful<br><br><img src=""paste-f64950d2f8d344c3e3bcc9a575a22cc22288005c.jpg"">"	memory-admin
Sist. Operativos Final	What is Belady's anomaly?	"Belady's anomaly is a term referring to the fact that as the frames available in RAM increases, the chances of a page fault occuring in SOME page replacement algorithms increases.<br><br>For example, in the FIFO page replacement algorithm:<br><br><img src=""paste-349d06f0db095894d2658a416b28ad3ff69270b8.jpg"">"	memory-admin
Sist. Operativos Final	What is the target of all page replacement algorithms?	The target is to reduce the number of page faults that occur when the CPU tries to access a given page&nbsp;	memory-admin
Sist. Operativos Final	What page replacement algorithms avoid Belady's anomaly and which ones don't? Why?	Algorithms that avoid the anomaly:<br><br>- Optimal: Theoretical benchmark used for comparison where an OS removes the page that'll be used farthest in the future. Produces the least page faults.<br><br>- LRU: Removes the page that was least recently used<br><br>They avoid the anomaly because they are stack-based algorithms that, when working with more frames, don't inadvertently evict pages that were being kept with fewer frames.&nbsp;<br><br>Algorithms that don't avoid it include FIFO, Most Recently Used and Random (evicts an arbitrary page)	memory-admin
Sist. Operativos Final	What is thrashing? (hiperpaginación)	It's a term that references the phenomenon where the system spends most of it's time handling page faults and swapping pages in and out of disk instead of executing processes	memory-admin
Sist. Operativos Final	How does a system minimise thrashing? (4)	Thrashing can happen when there's a high degree of multiprogramming or a lack of frames&nbsp;<br><br>Measures used to deal with thrashing:&nbsp;<br><br><b><u>Working set</u></b> model: Keeps whole process' working set in RAM (according to estimations on behalf of the OS), making it so that processes that are too big to execute their working set on available frames wait until space is sufficient<br><br><b><u>Page Fault Frequency</u></b>: Adjusts frames allocated to processes dynamically based on page fault rate<br><br>Load control: <u><b>limits</b></u> number of active processes<br><br><u><b>Local page</b></u> replacement: Makes it so that pages of a process can only be replaced by pages of the same process	memory-admin
Sist. Operativos Final	How does a system detect thrashing?	Indicators used by operating systems to detect thrashing:<br>1. High page fault rate<br>2. Low CPU utilization with high disk I/O activity<br>3. Active processes with working sets larger than available RAM	memory-admin
Sist. Operativos Final	What is equal frame allocation?	"When the system allocates an equal amount of frames to each process regardless of the amount of entries in it's page table. It's not a ""fair"" system in terms of performance&nbsp;&nbsp;"	memory-admin
Sist. Operativos Final	"""Proportional"" frame allocation"	Frames are distributed among processes proportional to the size of each process. For example,&nbsp;<br><br>Total available frames: 80<br><br>Process A:<br>Pages: 10<br>Allocated frames: 80*(10/40) = 20<br><br>Process B:&nbsp;<br>Pages: 30<br>Allocated frames: 80*(30/40) = 60	memory-admin
Sist. Operativos Final	"What does the term ""global page replacement"" reference?"	Global frame replacement is a policy where a page faulting process may replace any page in RAM according to the page replacement algorithm in use, not only pages belonging to itself. This increases flexibility and overall memory utilization, but allows processes to take frames from others, potentially increasing their page fault rate and causing unfairness.	memory-admin
Sist. Operativos Final	Name the fundamental property of a stack algorithm in page replacement	<div>A stack algorithm is a page replacement algorithm with this property:</div> <div>If a page would be in memory with n frames, it will also be in memory with n+1 frames.</div>	memory-admin
Sist. Operativos Final	When is FIFO &gt; LRU true? Name an example reference string	"FIFO can prove to be a superior PRA in comparison to LRU in part thanks to it's lower implementation overhead (no additional required data structures) and lesser associated metadata only in specific reference patterns.<br>One example where this proves true is for the reference string ""1 2 3 4 1 2 5 1 2 3 4 5"", assuming 3 frames<br><br>In this case, FIFO produces fewer PFs than LRU. This is purely coincidental however and as a rule of thumb LRU should be considered the optimal PRA *when compared to FIFO*"	memory-admin
Sist. Operativos Final	Virtual vs. Physical addresses	Virtual addresses are addresses that indicate a location within the virtual space of a process, that the CPU uses as an interface to interact with RAM with the help of the MMU<br><br>Physical addresses refer to locations in actual physical main memory and are obtained by different OS components depending on the address binding type employed:<br><ul><li>Compile-time binding: the compiler (happens if a base physical address is known a priori)</li><li>Load-time binding: the physical addresses are resolved by the loader</li><li>Execution-time binding: the MMU resolves the addresses using a given process' page table</li></ul>NOTE: The loader doesn't use page tables, not even if load-time binding is in use. This is because a process' page table is generated strictly at execution time alongside other process management data structures such as the PCB<br>	memory-admin
Sist. Operativos Final	When does a minor page fault occur?	A soft (minor) page fault occurs when the requested page is not currently mapped in the process’s virtual address space, but the page already exists somewhere in RAM, so the OS can resolve it without disk I/O. This can happen in two main scenarios:<br><br>- The page is in page cache (portion of RAM allocated to recently referenced pages)<br>- The page is currently in a different process's physical memory space&nbsp;	memory-admin
Sist. Operativos Final	What is copy-on-write?	Copy-on-write is a memory optimization technique used by operating systems to delay copying a memory page until it is actually written to by the reuqesting process. It allows multiple processes to share the same physical page in RAM until one of them writes to it.<br><br><i><b>It only applies to user-space memory spaces that are initially shared, like:</b></i><br><ul><li>Memory after fork()</li><li>When a process requests a private copy of a shared memory space</li></ul>	memory-admin
Sist. Operativos Final	When will a write-back not occur in the case of a page fault?	A page fault caused by a read request occurs when the process tries to access a code or file page not currently in RAM.<br> The OS loads the page into a free frame or replaces a clean (unmodified) page if needed. Only a read is required; no write-back is necessary	memory-admin
Sist. Operativos Final	"What is a ""constrained buffer""?"	A constrained buffer refers to a space in the OS' address space that a producing process writes data to and that a consuming process reads from<br><br><i>A use case could be one like the following:</i><br>A producing process writing to a queue of integers that are read and removed from the queue by a different consuming process that stores them in a text file	process-synch
Sist. Operativos Final	"What is the ""producer-consumer problem"" and what is it's solution?"	"The problem lies in ensuring that there is space in the bounded buffer before producing and that there are items to be read before consuming. Synchronization between processes is necessary to achieve this and prevent race conditions/data corruption<br><br>The solution to the producer-consumer problem is found in the operations surrounding the critical sections in which the buffer is accessed by the participating processes: P (try to decrease/wait) and V (increment/signal)<br><br>Alternatively, busy waiting can be used<br><br><img alt=""Semaphores, Nachos"" src=""oct02img1.gif"">"	process-synch
Sist. Operativos Final	What do wait() and signal() do?	They are operations that decrement (or try to) and increment a given semaphore respectively<br>When implemented in the context of bounded buffers, the semaphore value corresponds to the number of available slots at a given time<br>They are not syscalls<br><br>P and V must be executed atomically: they cannot be interrupted once they begin execution and they cannot execute simultaneously. They modify the value of only one semaphore at a time	process-synch
Sist. Operativos Final	Semaphore types and their typical use cases	<b>Counting semaphores:</b> control access to a resource with multiple instances, like a bounded buffer. Can take any non-negative int value and represents the number of available resources<br><br><b>Binary semaphore:</b> used for mutual exclusion and can take only the values of 0 and 1. It ensures that only one process enters the critical section at a time	process-synch
Sist. Operativos Final	What is busy waiting?	"Busy waiting is a less optimal solution to the producer-consumer problem. It works through the continuous check of a given condition, such as a full buffer or empty buffer in a loop before proceeding into it's critical section.&nbsp;<br><br>It is less optimal because it wastes CPU cycles by staying in the running state rather than sleeping, even if it isn't doing anything but checking a condition.<br><br><img alt=""Mutual Exclusion with Busy Waiting (Software approach)"" src=""sAuUaQxYMjJQa0VeAsgaaKRFbc7BLGTRd4cxas3Rq3oY9LTfjqwV4pa4gm5isVaCmWfKoJLP-HAARGtlI9BltbrBYYiGRCJAnlv-aJzBulZ2Uc9uu-s.png"">"	process-synch
Sist. Operativos Final	How does semaphore usage spare CPU resources?	"Semaphore usage spares CPU resources through the following mechanism, assuming a Linux based system and no busy waiting:<br><br>When a process performs a wait() operation and the semaphore value is 0, the process is put to sleep by the OS and placed in the semaphore's waiting queue. The process enters the waiting state and remains in main memory<br><br>When a different process performs a signal() operation on that same semaphore and releases the semaphore, the waiting process is then promptly ""woken up"" and once more put into the running state<br><br>By implementing syscalls and performing privileged operations, we evade busy waiting through temporary process suspension&nbsp;"	process-synch
Sist. Operativos Final	Producer-consumer problem vs. critical section problem	The critical section problem is a generic term that references the risk of race conditions in the case of two or more processes concurrently accessing shared resources. The solution is any mechanism that allows only one process to access the shared resource, ensures no process waits indefinitely and that allows progress<br><br>The producer-consumer problem is a specific instance of the critical section problem that applies specifically to shared buffers	process-synch
Sist. Operativos Final	What 3 requirements determine if a solution to the critical section problem is valid or not?	Bounded waiting: it's purpose lies in making sure that a process only waits so long to execute it's critical section. It makes it so that no process is starved. For example: semaphores with waiting queues, a lock with an associated FIFO queue&nbsp;<br><br>Mutual exclusion: Only one process can enter it's critical section at a time<br><br>Progress: If no process is in the critical section and some wish to enter, the choice of the next process must not be postponed indefinitely, and only processes that are trying to enter can participate in that decision.<br><br>(BPM)	process-synch
Sist. Operativos Final	Solutions that exist for the consumer-producer problem:	<ul><li>Peterson's algorithm (implements bounded waiting)</li><li>Semaphores (implemented with the use of wait() and signal())</li><li>Interrupt disabling on systems with a single CPU core (not recommended due to the fact it hurts responsiveness and blocks preemption)</li></ul>	process-synch
Sist. Operativos Final	Preemption vs interruption	Preemption: The forceful seizure of CPU resources from one process to allocate them to another one as a result of scheduling policies (for example: round robin, process requests I/O)<br><br>Interruption: A signal (either from hardware or software) that halts the CPU's current activities to give attention to something more urgent<br><br>Relation: interrupts often cause preemption. For example, hardware interrupt -&gt; CPU jumps to interrupt handler in kernel mode -&gt; handler finishes -&gt; scheduler makes decision -&gt; CPU either resumes the same process or preempts the process	scheduling
Sist. Operativos Final	What is Peterson's algorithm and how does it work?	"It is a solution to the critical section problem (where only one process at a time can access a shared resource) specifically for two processes.&nbsp;<br><br>It uses shared variables and busy waiting<br><br>Shared variables: an array of two flags which indicate if a process is trying to enter it's critical section or not and a 'turn' variable, that acts as a tie-breaker if both processes wish to enter their critical sections<br><br><div><ul><li>Flags = ""I want to enter the critical section""</li><li>Turn = ""Okay, you go first if you also want to enter; I’ll wait""</li></ul></div> <br><img src=""paste-983725e7fbedc7ccc16ca228625bb22e8952b061.jpg""><br>"	process-synch
Sist. Operativos Final	What is a deadlock? What four conditions must exist for one to happen?	"<span style=""color: rgb(39, 50, 57);"">Deadlock is a state in an operating system where two or more processes are stuck forever because each is waiting for a resource held by another. It happens only when four conditions exist: mutual exclusion, hold and wait, no preemption, and circular wait.<br></span><div><ul><li>Mutual Exclusion:<span style=""color: inherit; font-weight: inherit;"">&nbsp;Only one process can use a resource at any given time i.e. the resources are non-sharable.</span></li><li>Hold and Wait:<span style=""color: inherit; font-weight: inherit;"">&nbsp;A process is holding at least one resource at a time and is waiting to acquire other resources held by some other process.</span></li><li>No Preemption:<span style=""color: inherit; font-weight: inherit;"">&nbsp;A resource cannot be taken from a process unless the process releases the resource.&nbsp;</span></li><li>Circular Wait:<span style=""color: inherit; font-weight: inherit;"">&nbsp;set of processes are waiting for each other in a circular fashion.</span></li></ul></div><img alt=""Introduction of Deadlock in Operating System - GeeksforGeeks"" src=""deadlock.png""><br>"	deadlocks
Sist. Operativos Final	What resources are involved in the occurence of deadlocks?	Non-preemptible resources (CPU time is preemptible) such as semaphores and I/O devices.&nbsp;<br><br>They are non-preemptible because:<br>- Accesses to semaphores represent atomic actions that must not be preempted by definition<br>- If we preempt output to or input from I/O devices this could lead to, for example, undefined behaviour or inconsistent data (example: in an output request to a network card, preemption could lead to incomplete data being sent over a network)&nbsp;	deadlocks
Sist. Operativos Final	What is a process scheduler? What types of process schedulers exist?	"<div><div>A process scheduler is a fundamental component integrated into a computer system's OS (the kernel, specifically) that manages when processes execute and for how long. There are 3 types:<br></div><ul><li>Long-term scheduler: Admits processes into the system and controls the degree of multiprogramming. It runs infrequently and helps balance CPU-bound and I/O-bound workloads.</li><li>Short-term scheduler: Selects a process from the ready queue and allocates the CPU according to a scheduling policy such as FCFS, SJF, or round-robin.</li><li>Medium-term scheduler: Manages swapping by suspending processes and moving them between RAM and secondary storage to relieve memory pressure.</li></ul></div>

<br><img alt=""seven_state"" src=""seven_state.webp""><br>"	scheduling
Sist. Operativos Final	Does deadlock imply starvation? Does starvation imply deadlock?	A deadlock does imply starvation since in the occurence of one, by definition, processes are deprived of resources that they need in order to execute to completion<br>Starvation itself however does not imply a deadlock, since processes can also be starved due to reasons relating to process scheduling policies and process priorities	deadlocks
Sist. Operativos Final	What is a dispatcher and how does it differ from a scheduler?	"The dispatcher is a program native to the operating system that takes over after the *Short-term* scheduler makes a decision as to what process should be taken from the ready queue and allocated CPU resources.<br>It has 3 key responsibilities: <br><ol><li>Context switching (transitioning from one process to another, saving process states in their corresponding PCBs) </li><li>Switching from kernel mode to user mode and viceversa accordingly</li><li>Retrieving previous process states by loading their according values onto the CPU's registers using their PCBs</li></ol><br><img alt=""Process Schedulers in Operating System - GeeksforGeeks"" src=""dispatcher.webp""><br><img alt=""What is dispatch latency in operating systems – IT Release"" src=""What-is-dispatch-latency-in-operating-systems.jpg""><br>"	scheduling
Sist. Operativos Final	Briefly name the CPU Scheduling Algorithms that exist	"Preemptive:<br><ul><li>- Priority Scheduling</li><li>- Shortest Job (CPU burst time) Remaining first</li><li>- Longest Job Remaining first</li><li>- Round Robin</li></ul>---<br>Non-preemptive:&nbsp;<br><ul><li>- First Come First Serve</li><li>- Shortest Job First</li><li>- Longest Job First</li><li>- Highest Response Ratio Next (HRRN prioritises processes based on how long they have been waiting and how short they are, shorter processes that have waited for longer taking precedence over others)<br><br></li></ul><img alt=""CPU-Scheduling"" src=""CPU-Scheduling.png""><br>"	scheduling
Sist. Operativos Final	How is priority determined in the context of Preemptive Priority Scheduling?	Priority can be determined both dynamically and statically (either as processes wait and execute or when they are loaded respectively)<br><br>In dynamic priority adjustement/assignment, determining factors include wait time of a process as well as CPU usage of that process<br>In both the cases of static and dynamic priority determination the type of process (kernel, real time, interactive processes/background tasks) plays a role<br><br>NOTE: Dynamic priorities can also be used in the context of non-preemptive process scheduling if a process has been waiting for too long, for example. Priority can be adjusted while a process is in the waiting queue	scheduling
Sist. Operativos Final	What are the selection criteria that determine the best CPU scheduling algorithm in a given case? (6)	<div><ul><li><u><b>CPU utilization:</b></u> How much time does the CPU spend doing actual helpful work? <i>(If a scheduling algorithm were to be poorly selected, the CPU would spend more time performing context switches or sitting idly as a process requests I/O than actually executing instructions)</i></li><li><u><b>Throughput:</b></u> How many processes are completed per unit of time? (ms, s)</li><li><u><b>Turnaround time:</b></u>&nbsp;How long does a process take from submission to completion? (submission is handled by the long-term scheduler)</li><li><u><b>Waiting time:</b></u> How long does a process spend waiting in the ready queue?</li><li><u><b>Response time</b></u>: How long does it take until the process FIRST gets the CPU? <i>(waiting time can refer to how long a process spends waiting after it is suspended too)</i></li><li><u><b>Fairness:</b></u> Do some processes dominate the CPU while others starve?</li></ul></div>     	scheduling
Sist. Operativos Final	How do a long-term scheduler and medium-term scheduler differ?	"- A long term-scheduler decides admission into memory initially<br>- The medium-term scheduler however, temporarily swaps processes in and out of memory AFTER they have already been admitted<br><br>The medium term scheduler is not in charge of handling terminated processes, only processes that are not done executing<br><br><img src=""paste-e2e14ddb6743af009626cf387d9ab2048d75daa6.jpg"">"	scheduling
Sist. Operativos Final	Draw the states associated with a progress, indicating the associated schedulers, secondary and main memory and the events/OS actions that cause transitions between states of a process. Where does the dispatcher intervene?	"<img alt=""States of a Process in Operating Systems - GeeksforGeeks"" src=""states_modified.png"">"	processes
Sist. Operativos Final	What is an interrupt vector table?	"A interrupt vector table can be thought of conceptually as an array of addresses corresponding to different interrupt handlers associated with their corresponding interrupt vector numbers (indexes)<br><br>It lives in the kernel address space<br><br><img alt=""inttable.jpg"" src=""inttable.jpg"">"	interrupts-and-traps
Sist. Operativos Final	Trap vs. syscall vs. software interrupt	"<b><u>Trap:</u></b>&nbsp;Software-generated interrupt caused either by an error (for example, division by zero or invalid memory access) or by a specific request from a user program that an operating-system service be performed.<br><u><b><br>Syscall</b></u>: Operation that triggers a software interrupt and allows user applications to request services from the operating system, such as:<br><ul><li>Reading/writing from a file</li><li>Allocating memory</li></ul><br><div><b><u>Software interrupt:</u>&nbsp;</b>Any signal generated by software either because the program made a request for a service (for example, a system call) or because it encountered an error (such as divide‑by‑zero).<br><br>-<br>Summary:&nbsp;</div><div>The way they relate is that they all are mechanisms that transfer control from the currently running process to the kernel briefly to execute a given interrupt handler (based on the interrupt vector number raised). System calls however are special in that they are written into a process' program (for example: open, fwrite, fread, malloc...)</div><div>Sources seem to make little distinction between SW interrupts and traps<br><br></div><div><img alt=""frame_11"" src=""frame_11.webp""><br></div>"	interrupts-and-traps
Sist. Operativos Final	Paging vs. Demand Paging. Benefits of demand paging	"Paging is a memory management method in which RAM is divided into fixed-size blocks called frames and processes' logical memory is divided into pages of the *same size*<br><br><ul><li>Traditional paging: All process pages are loaded into RAM at once upon admission by the long-term scheduler</li><li>Demand paging: Pages are loaded into RAM when they are actually needed (on page fault). The medium-term scheduler only allocates frames when the processor tries to access a page that is not yet in memory</li></ul><br><div><img src=""paste-b2679bad51151b510f21d7cf61b7579e576ef945.jpg""><br></div>"	memory-admin
Sist. Operativos Final	"What is the ""best case scenario"" in the case of a page fault?"	"A ""soft"" page fault represents the best case scenario and happens when the processor requests access to a page that is *already loaded into RAM*, just not in the currently running process' memory space. It can be in either:<br><br>- Page cache (small, quick-access memory that stores recently referenced pages)<br>- A different process' memory space"	memory-admin
Sist. Operativos Final	Are syscalls more complex in monolithic OS or in a layered OS?	In a layered OS, a system call incurs more overhead because the request must pass through multiple layers before reaching the kernel services. In a monolithic OS, kernel services are directly accessible, so system calls are faster and involve less context switching or layer traversal.	interrupts-and-traps
Sist. Operativos Final	Which of the following hardware mechanisms is not a requirement to build a multiprogrammed operating system with user protection? Explain your answer.<br> a) Virtual memory<br> b) Memory protection<br> c) I/O instructions that can only be executed in kernel mode<br> d) Two modes of operation: kernel and user	"Virtual memory: It's main purpose lies in process isolation, use of RAM through paging/segmentation and enabling swapping (and therefore execution of processes larger than RAM). It helps with protection but it is not a *requirement*. Protection can be achieved through other means such as with the use of limit registers<br><br>Reasoning: <br>- Memory protection ensures process integrity and that other processes don't overwrite other process' data, either by mistake or maliciously <br>- Privileged I/O instructions: If any user process were able to perform I/O operations freely, user processes would be able to fetch from disk information reserved for other users of the system with no regard for system permissions, violating the ""user protection"" principle <br>- Operation modes: Kernel and user. Without permission management and access limitatation, memory protection and protection of privileged instructions would not be possible"	memory-admin
Sist. Operativos Final	How is a syscall performed?	<div><ol><li>A system call is part of the process’s code (e.g., open())</li><li>When called, it triggers a software interrupt with a specific interrupt number</li><li>Control is transferred to the kernel, which executes the corresponding interrupt handler. The kernel uses the interrupt number as an index into the interrupt vector table</li><li>The kernel performs the requested privileged operation safely</li><li>Control is returned to the user process after completion</li></ol></div>    	interrupts-and-traps
Sist. Operativos Final	Multiprogramming vs. multiprocessing vs. time sharing vs. multitasking	"<ul><li><i><b>Multiprogramming</b></i> implements time sharing and enables multitasking by rapidly switching between contexts, saving and loading process states and loading multiple processes onto RAM (either whole or in seguments/pages depending on the memory management schema)</li><li><i><b>Multiprocessing</b></i> is a characteristic of systems with multiple processors or multiple processor cores that allows for *true* concurrency. Each processor/core can either satisfy the multiprogramming characteristic or not</li><li><i><b>Time sharing</b></i>: Processes are rapidly allocated and de-allocated CPU resources from admission to completion so that they can run in a seemingly ""concurrent"" manner</li><li><i><b>Multitasking</b></i>: The characteristic of some computer systems that allows them to have multiple tasks running concurrently. Can be achieved using multiprogramming, multiprocessing or both</li></ul>"	os-structure
Sist. Operativos Final	What are threads? What resources do they share and which ones do they not?	"Threads are the smallest unit of CPU execution within a process<br><br>Resources shared among threads of the same process:<br>- Address space (code, data, heap)<br>- Open files<br>- Global variables<br>- Shared memory, semaphores<br><br>NOT shared:<br>- Program counter<br>- Registers (program counter, stack pointer...)<br>- Stack (for local variables and function calls)<br><br><img alt=""Operating Systems: Threads"" src=""4_01_ThreadDiagram.jpg"">"	processes
Sist. Operativos Final	What is the program counter?	The program counter is a register in the CPU that holds the ***address*** of the next instruction to execute by a thread or process	os-structure
Sist. Operativos Final	What is the stack pointer? What are local variables and return addresses?	<div>The stack pointer (SP) is a CPU register that points to the top of the current stack frame for a thread or process</div> <div><ul><li>It tracks where local variables, function parameters, and return addresses are stored in the stack</li><li>Each thread has its own stack pointer, so its stack operations (push/pop) do not interfere with other threads, even if they share the same process memory</li></ul></div><div>- Local variables: for example, parameters in function definitions or variables declared within function definitions which scope does not exceed *that* function</div><div><br></div><div>- Return addresses: where do we return to when we are done executing this function?</div> 	processes
Sist. Operativos Final	How do threads and processes differ? Briefly explain IPC	Threads of the same process share parent process resources by default, allowing them to communicate with more ease and lesser associated overhead<br><br>Meanwhile, in order to communicate with one and other, processes need to establish some form of IPC<br>In order to achieve this the kernel needs to allocate to the processes a space in memory that they can share with one and other and must access in a manner that is mutually exclusive<br><br>Shared memory is managed by programs through syscalls and functions, such as shm_open, mmap (maps shared memory object into process' virtual space) and shm_unlink (removes shared memory object from system).&nbsp; Alternatively, IPC can also be implemented through a message-passing system<br><br>In addition to differing in how they communicate, threads are lighter (don't require their own memory space and have lesser associated metadata) and have a lifetime that is not independent, that is dependent on the parent process	processes
Sist. Operativos Final	Why does message-passing have higher associated overhead in IPC?	"Processes share information with one and other through the passing of messages through the kernel<br><br>It has higher associated overhead when compared to shared memory IPC due to the higher kernel interference. In shared memory systems, the kernel only interferes when the shared memory space is created, mapped onto a process' memory space, unmapped and removed from the system. Meanwhile in message passing, the kernel must create, close, remove the communication channel and also manage the sending and receiving of messages<br><br><img alt=""Message_Passing_"" src=""Message_Passing_.webp""><br><img alt=""Shared_Memory"" src=""Shared_Memory.webp"">"	ipc
Sist. Operativos Final	What is a stack frame?	"A stack frame is to the stack memory of a process/thread what a checkout divider is to a checkout counter in a supermarket. It keeps each function call's data seperate to that of other function calls<br>It is important to keep in mind that the stack grows from higher addresses to lower addresses<br><br><br><img alt=""stack-contents"" src=""stack_contents.png"">"	os-structure
Sist. Operativos Final	Different tools used by message-passing systems in IPC	"<div><ul><li><u><b>Pipes</b></u> are unidirectional communication structures handled by the kernel which are typically used for parent-child processes. Data is sent as a continuous sequence of bytes and not as discrete messages; there are no inherent boundaries between ""writes"". Limited to processes on the same host machine<br><u><b>Sockets</b></u>: Bidirectional communication endpoints managed and assigned to processes by the kernel. In the cases of networked communication across two remote devices, they are identified by IP addresses with port numbers appended. Depending on whether TCP or UDP is in effect (protocols) communication using sockets can be stream-based or message-based respectively<br><u><b>Message queues/mailboxes</b></u> implemented using message queues: Enable communication between multiple processes through the use of a kernel-managed data structure. Sending/writing processes target the specific mailbox or queue and reading/recepient processes target messages by type, for example. They can be either unidirectional or bidirectional and use discrete messages to communicate data<br></li></ul><div><img alt=""Johnny Huang » Linux IPC with Pipes"" src=""pipe1.png""></div><div><img alt=""Message Queues"" src=""multiple_message_queue.jpg""></div><div><img alt=""Using sockets for inter-process communication — Computer Networking :  Principles, Protocols and Practice"" src=""tikz-dc6fb23b8259bd2ae0023eb24a79fa9dea4819f1.png"">&nbsp;<br></div></div>  "	ipc
Sist. Operativos Final	What is the PCB and what is it's purpose? (8 items)	"It is a data structure used by the OS to manage processes generated upon process loading onto RAM, it stores:<br><br>- Process state (new, ready, running, waiting/blocking, terminated, suspended blocked, suspended ready)<br>- PID &amp; PPID<br>- CPU registers: Program counter, stack pointer, general-purpose registers (operands, operators, intermediate values)<br>- Memory-management info: base and limit registers, page tables or segment tables (depends on memory management schema)<br>- Information about CPU usage, execution time and time limits (measured from process admission into main memory by the long-term scheduler) (Accounting info)<br>- CPU scheduling info: Process priority, process type<br>- I/O status info: list of open files and peripheral devices allocated to the process<br>- Pointers to child processes' PCBs and parent process' PCBs<br><br><img alt=""What is Process Control Block (PCB) in Operating System? - Binary Terms"" src=""Process-Control-Block.jpg"">"	processes
Sist. Operativos Final	Indicate which of the following operations is not performed by the dispatcher:<br> a) Restore the user registers with the values stored in the process table.<br> b) Restore the program counter.<br> c) Restore the pointer to the process’s page table.<br> d) Restore the memory image of a process.	d) Restore the memory image of a process. That is the job of the loader<br><br>a), b) and c) These are all processes fundamental to the dispatcher's job: restore the state of a process with the help of it's associated PCB, created upon it's loading onto main memory	processes
Sist. Operativos Final	What is system overhead (sobrecarga del sistema) in OS?	System overhead in operating systems is the time and resources that the operating system consumes to manage processes, instead of executing useful user work. In place of allocating time to the execution of useful processes the OS is performing operations such as:<br><br>- Changing process states<br>- Performing context switches<br>- Handling syscalls &amp; interrupts	scheduling
Sist. Operativos Final	What is a zombie?	A zombie is a process that has terminated but still has an associated PCB stored in main memory (RAM), within the kernel's memory space. This can happen if it's parent does not read it's exit status by performing a wait() operation on it<br>Accumulated zombies can inhibit the creation of new processes and consume small but not entirely insignificant amounts of memory<br><br>Accumulated zombies inhibit new process creation because the OS keeps track of the amount of active processes at a time by checking the amount of PCBs in the kernel's memory space at a given time and enforces a limited amount of active processes per user	
Sist. Operativos Final	What is the loader? Describe briefly what it does	The loader is an OS component responsible for loading a program's executable image from disk into RAM for execution (executable images = segments that can be paginated)<br><br>It does the following:<br><ul><li>Allocates memory in RAM for segments</li><li>IF the computer system uses load-time physical address linking, it resolves addresses for offsets/virtual addresses generated at compile time by the CPU</li><li>Sets up program counter and stack pointer for a process upon first execution</li></ul><div>NOTE: It is seperate from the memory manager. The memory manager provides and manages memory, the loader loads the process' image onto RAM</div>	os-structure
Sist. Operativos Final	Briefly explain HRRN	"A scheduler that follows the non-preemptive scheduling algorithm Highest Response Ratio Next chooses processes from the ready queue based on their response ratio: the one with the highest response ratio is selected next. The shorter the next CPU burst and longer the waiting time, the sooner the process gets the CPU<br><br>The response ratio is computed as follows:<br><img src=""paste-e0b5e237666834354a73101de34a598193d800c5.jpg"">"	os-structure
Sist. Operativos Final	Is a process' page table and PCB created upon first execution or upon loading onto RAM?	A process’s PCB and page table are created when the process is loaded into memory, not necessarily at first execution.	os-structure
Sist. Operativos Final	Consequences of disabling interrupts on the computer system's clock	Disabling interrupts won't have any effects on the physical component that is the system clock itself, but it will affect the computer system's behaviour in response to it in the following manner:&nbsp;<br><ul><li>Timer interrupts will be ignored and <u><i>preemptive scheduling</i></u> will stop</li><li>User processes will make system calls, but <u><i>most sycalls</i></u> will not be satisfied given that most of them rely on interrupts- temporary halting of the CPU's current execution to allocate resources to the kernel. <u><i>I/O operations</i></u> will be affected as well as operations such as wait(). Not to mention mechanisms that enable <u><i>IPC</i></u> such as shared memory or message sending will not work properly</li></ul>	interrupts-and-traps
Sist. Operativos Final	"<img src=""paste-a17d60eb36efc8b6a3126f5b0e4b19fa3cc4293d.jpg"">"	En este esquema, se provocaria una perdida de señal en el caso que el escritor señalice al proceso lector mientras este esta leyendo y no esperando una señal. Antes de escribir y señalizar, deberia haber algun tipo de mecanismo en el escritor para que este se asegure que el lector no este leyendo	process-synch
Sist. Operativos Final	Internal vs. external fragmentation	"<b><u>Internal fragmentation occurs in:</u></b><br>- Paging (non-contiguous memory allocation)<br>- Fixed-size partition schemas (contiguous memory allocation, paging is a kind of fixed-size partition memory management schema)<br><ul><li>It refers to what happens when a process or virtual page doesn't ""fill"" the entire partition/frame allocated to it in physical RAM; the allocated partition is bigger than it and thus RAM resources go wasted</li></ul><br><u><b>External fragmentation happens in:</b></u><br>- Variable-size partition schemas (contiguous, includes segmented memory management)<br>- Segmentation (non-contiguous)<br><ul><li>When partitions are made to dynamically accommodate either whole processes or memory segments of these processes, when these segments/processes are deallocated RAM, they leave behind ""holes"" where memory resources are unused. Some are too small to accomodate processes in the ready queue or their corresponding segments. When this happens, external fragmentation is produced<br><br></li></ul><img alt=""Internal Fragmentation"" src=""Untitled-Diagram-146.png"">&nbsp;<img alt=""External Fragmentation"" src=""2581.png""><br>"	memory-admin
Sist. Operativos Final	"What OS component is in charge of selecting a ""victim page"" in a page replacement algorithm?"	<div>The operating system’s memory manager (sometimes called the page replacement module) is responsible for selecting a victim page when a page replacement is needed:</div> <div><ul><li>When a page fault occurs and there is no free frame, the page replacement algorithm (e.g., FIFO, LRU) is invoked.</li><li>The memory manager uses the chosen algorithm to identify a page in RAM to evict (the “victim”) to make room for the new page.</li><li>Once the victim page is selected, if it is dirty, it is written back to disk; otherwise, it is simply overwritten.</li></ul></div>  	memory-admin
Sist. Operativos Final	What is the linkage editor/linker?	"OS component that combines multiple object modules into a single executable, scanning system libraries to pull in the modules and functions required for the code to be successfully executed<br><ul><li>Object module: Program compiled by the C compiler (e.g. GCC) with symbolic references to functions dependant on system libraries, for example</li><li>Load module: Executable machine code with symbolic references resolved</li></ul><br><u>Program ""linking"" occurs at ""link time"" and produces the final executable that is *actually* runnable. It intervenes upon executing the following command, for example:</u><br><i>gcc program.c dependency.c -o final-project.c</i><br>or<br><i>gcc program.c -lm -o final-project</i>"	os-structure
Sist. Operativos Final	How can OS be categorised according to how they manage programs?	Assuming only one CPU/processing core:<br><ul><li>Batch operating systems: Manage programs and data sequentially, in batches, without requesting or processing user interactions. Traditional batch systems are non-preemptive</li><li>Time-sharing OS: Manage programs in a seemingly 'simultaneous' manner. Designed for interactive use, incorporate time sharing to enable multitasking and give the illusion of simultaneous execution</li></ul>	processes
Sist. Operativos Final	What does an execution context change look like?	"<img src=""paste-62dacb5bbb2ffdb3d654e82570e9d0a11f845044.jpg"">"	processes
Sist. Operativos Final	Queues associated with process scheduling	"- Process queue: all the processes within the system<br>- Ready queue: processes residing in RAM ready for execution waiting to be allocated CPU resources<br>- Device queue: a queue corresponding to any given peripheral device that is maintained by the OS and holds processes waiting for access to said I/O device<br><br><img src=""paste-066860293fdd1fd01de2a40241dc46901b1045ee.jpg"">"	processes
Sist. Operativos Final	Process types according to time distribution	- CPU bound processes: Spend more time quantums using the CPU to perform mathematical and logical operations on data to produce results. Includes compilation of SW projects, scientific simulations. Limited by processor speed (cache size, number of cores, clock speed Hz)<br><br>- I/O bound: Spend more time performing I/O operations and perform few mathematical and logical operations. Includes programs that write files to disk, web browsing (outputs network requests, receives responses). Limited by I/O device speed	processes
Sist. Operativos Final	How are processes created in Linux-based OS?	"<ul><li><b><u>Parent</u></b> process executes the <u><b>fork()</b></u> syscall</li><li>Created child <u><b>gets a copy of the parent's memory</b></u> and <u><b>resources</b></u>. They are shared until either the parent or child requests to write to a page: when this happens, the page is copied to the requesting process' memory space (<b><u>copy-on-write</u></b>)</li><li>Child executes <u><b>exec()</b></u>, replacing it's user-space memory image including code segment, data segments (e.g. global variables), heap (malloc()) and stack (local variables). The only memory that remains is that managed by the kernel, which includes open file descriptors and shared memory created via shmget()</li></ul><br>Example: if before executing fork() the parent process performs an fopen() operation on a file, the child process will keep the returned file descriptor even after performing exec() unless explicitly specified otherwise<br><br><img src=""11150.png""><img src=""12127.png""><br>"	processes
Sist. Operativos Final	Describe the sequence of actions that occurs upon child process termination	"<ul><li>The child process performs it's last operation and then performs the exit() syscall to transition to the terminated status</li><li>The parent process reads it's exit status and deletes the child process' PCB from RAM through the wait() operation</li></ul><div><br></div><div><img alt=""Process Creation and Deletions in Operating Systems - GeeksforGeeks"" src=""unix_process_creation-660.webp""><br></div>"	processes
Sist. Operativos Final	What operations/syscalls are used by processes if message-sending IPC through message queues is being implemented?	"msgget() creates or opens a message queue, msgsnd() sends a message, msgrcv() receives a message and msgctl() deletes a message queue<br><br><img alt=""Message_Passing_"" src=""Message_Passing_.webp"">"	ipc
Sist. Operativos Final	How does IPC through mailboxes and through shared memory differ?	Mailboxes can be thought of as bidirectional message queues.<br>Mailboxes are data structures stored in the kernel and differ from shared memory in the sense that <b><u>they do not live in user memory space</u></b> and, in order to be written to (send()) and read from (receive()),&nbsp;<b><u>the kernel *must* interfere</u></b>. The mentioned syscalls rely on traps (i.e. SW interrupts), which implies more overhead due to context switching<br><br>Shared memory however just refers to a kernel-created space in RAM (specifically in the creating process' memory space) that both processes can access (although only one at a time with the use of busy waiting or synchronization primitives like semaphores) to write to and read from. <u><b>The kernel only interferes when the shared memory space is created and deleted</b></u>	ipc
Sist. Operativos Final	Direct vs. indirect communication (message-sending IPC)<div>Why are sockets and pipes NOT direct message-sending IPC?</div>	"<div>In direct message passing IPC, the sender explicitly names the receiver and the receiver explicitly names the sender. The communication primitives use process identifiers (library functions that wrap syscalls, syscalls)<br><br></div>
<div>In indirect message passing IPC, processes do not name each other. They name a kernel managed communication object such as a mailbox, pipe, message queue, or socket.<br><br></div><div>Sockets and pipes are not direct IPC because they use intermediate communication endpoints rather than addressing specific processes</div><div><br></div><div>Note: Indirect IPC implemented through a single maibox is insufficient for targeting specific receivers in multi-process IPC</div><div><br></div><div>Socket syscalls: https://man7.org/linux/man-pages/man2/send.2.html</div>"	ipc
Sist. Operativos Final	Why is message-passing easier to implement in OS? (in comparison to shared memory)	Mainly because semaphores and mutexes need not be created and managed since race conditions are prevented through kernel management and atomicity enforcement rather than synchronization primitives. In shared memory, the kernel cannot directly control access to the shared memory region; in message-passing, the contrary is true&nbsp;	ipc
Sist. Operativos Final	Use case of message-sending IPC	<i><u>Message-sending reigns as the only option for IPC in the following scenarios:</u></i>&nbsp;<br><br>- Implementation of distributed systems (e.g.: interconnected servers within a data center, the backbones of cloud services)<br>- Devices in two different LANs connected through a network that need to exchange data<br>- Communication between decoupled processes (processes that operate completely indepedently from one and other, that do not share memory)&nbsp;	ipc
Sist. Operativos Final	What are sockets? How are they identified? Use cases? (IPC)	"Sockets are communication endpoints implemented by a pair of communicating processes (one each). They are identified by IP addresses concatenated with a port number<br><br>They are used in processes implementing a client-server architecture: the <b><u>server waits</u></b> for incoming requests on a specified port which are <u><b>sent by a client</b></u> or set of clients<br><br><img alt=""Socket-in-Computer-Network-2"" src=""Socket-in-Computer-Network-2.webp"">"	ipc
Sist. Operativos Final	How do sockets work?	- The serving process performs a syscall to request a socket be assigned to it by the OS, with an associated identifier (IP address + port) and waits for incoming requests<br>- The client process does the same in order to send requests to a given server<br>- The client specifies the server's IP and port in the connect() syscall, requesting a connection<br>- A three-way handshake is performed (beyond the scope of this subject lol) and a connection is established	ipc
Sist. Operativos Final	What is dispatch latency?	Dispatch latency is the measure of time associated to the delay in a context change that the dispatcher performs:<br>- It refers to the time it takes the dispatcher to save the state of the process currently running to it's PCB, switch from user to kernel mode or viceversa (if necessary) and reload the CPU registers of the process transitioning from ready to running	scheduling
Sist. Operativos Final	Can batch systems integrate I/O?	<div>Yes. Batch systems can integrate I/O devices such as sensors, displays, and secondary memory, but the I/O is non interactive<br></div>	os-structure
Sist. Operativos Final	Why is disabling preemption insufficient as a solution to the CS problem in comparison to interrupt disabling?&nbsp;	Preemption is insufficient because a *hardware* interrupt can cause a process to be interrupted halfway through critical section execution, and say the handler enters that critical section, that'd potentially cause a race condition anyway even if preemption isn't allowed in the OS	
Sist. Operativos Final	"What is a ""time quantum"" in process scheduling?"	A time quantum is the amount of time a process is allowed to run on the CPU in a given scheduling algorithm before it is deallocated processor resources	scheduling
Sist. Operativos Final	Hard real-time systems vs soft real-time systems- what is a deadline?	"<ul><li>Hard real-time systems: Critical processes *must* finish running before a defined ""deadline"" or span of time. Missing a deadline is catastrophic and could imply injury or death; <i>an operation performed after it's deadline is as good as no operation done at all</i>. E.g.: flight control systems. More verbose example below</li><li>Soft real-time systems: No guarantee is given as to when a critical process will finish executing, the only guarantee is that a critical process will receive priority over other less critical processes in the ready queue</li></ul><br>A deadline is a defined slice of time within which a process will finish execution upon admission into the ready queue<br><br><img src=""paste-67fe72261d60b3b68006899c362435eb64197905.jpg""><img src=""paste-84e2e7c7d5c5a599ba2007b2319c2c3ea7372d76.jpg""><br>"	scheduling
Sist. Operativos Final	Event latency vs. interrupt latency vs. dispatch latency	<ul><li><b><u>Event latency</u></b> is the amount of time from when an event occurs (for example, user presses a key) to when that event is serviced (e.g. a window opens)</li><li><b><u>Interrupt latency</u></b>: Time elapsed between an interrupt being triggered and it's associated interrupt handler (i.e. ISR) running</li><li><b><u>Dispatch latency</u></b>: Time required for the dispatcher to stop one process and start another (an interrupt handler, for example)</li></ul>	scheduling
Sist. Operativos Final	Time-sharing systems vs. real-time systems	"Time-sharing systems and real-time systems differ mainly in their goals:<br>- Time-sharing systems use preemption to ensure a system is responsive and fair and for the enforcement of quantum limits (e.g.: Desktop OS)<br><br>- Real-time systems use preemption to ensure a system executes critical processes within a certain amount of time/quicker than less critical processes. They can be categorised into two different types: <br><ul><li>Hard real-time: Critical processes must run before a given time quantum and deadline misses are intolerable (e.g.: flight control systems) </li><li>Soft real-time: Critical processes are given priority over processes that are less critical, but deadlines are ""preferred"" and occasional misses are tolerated (e.g. input handling is prioritised over for example, booting/saving a session in a videogame)</li></ul><div>Soft RTOS vs time-sharing systems:<br><img src=""paste-530e8288a566f51d69bb9d3f81906fc3917e02d6.jpg""><br></div>"	scheduling
Sist. Operativos Final	Name the 4 fundamental characteristics of threads	"- They are <b><u>lightweight</u></b> execution units within a process<br>- They <b><u>share the same address space and resources</u></b> (code, heap, data segments, open files and I/O devices)<br>- They have their <b><u>own CPU registers and stack</u></b><br>- They <u><b>enable concurrent execution within applications</b></u><br><br><img src=""4_01_ThreadDiagram.jpg"">"	threads
Sist. Operativos Final	What is a user library?	A user library is a collection of code that runs in user address space inside a process. User libraries provide reusable functionality to programs	threads
Sist. Operativos Final	User-level vs. kernel-level threads	<div><i><u><b>User-level threads:</b></u></i></div><div>- <b><u>Managed</u></b>, created and scheduled <u><b>by user libraries</b></u> (code executed in user mode within the user process' address space), such as the JVM in the case of Java-written programs using 'virtual threads'</div><div>- <u><b>Unknown to the kernel</b></u>, known to their corresponding user library</div><div>- <u><b>Reside in the address space of the user process</b></u> they belong to</div><div>- Creating them and '<b><u>context-swapping</u></b>' between them is <u><b>fast</b></u> because it doesn't require kernel intervention<br><br></div><div></div><div><u><b>Kernel-level threads:</b></u></div><div>- <u><b>Managed by the kernel</b></u></div><div>- <b><u>Known to the kernel</u></b></div><div>- Supposing a user-level thread is mapped out onto one at a given time, the <u><b>code it will execute will belong to the corresponding user process' address space</b></u></div><div>- <u><b>Creating them is a slower process</b></u> relative to that of user-level threads, since when a user process creates a thread (e.g. using pthread_create() in a C program) the CPU must switch to kernel mode temporarily, causing overhead</div>	threads
Sist. Operativos Final	Explain an important reason as to why the implementation of user and kernel level threads matters in the context of concurrency in computer systems	User-level threads exist to allow for efficient sharing of resources within a process. For instance, if a process has five user-level threads, and threads 1–3 are mapped onto kernel-level thread 1 while threads 4–5 are mapped onto kernel-level thread 2, the process can perform two tasks concurrently on a multi-core system. This setup reduces the overhead of context switching among user-level threads (e.g. user-level context switch between threads 1 and 2 or between threads 4 and 5) while still allowing true parallel execution across kernel-level threads. Kernel-level threads exist to interface with the operating system, enabling scheduling, preemption, and handling of blocking operations. The distinction between the two types of threads allows systems to combine lightweight concurrency with scalable parallelism.	threads
Sist. Operativos Final	What are virtual threads in the context of Java?	"Virtual threads in Java are threads known only to the JVM, not the kernel, which are ""carried"" by carrier threads, i.e. kernel threads that virtual threads are mapped out onto<br><br><i>If virtual threads 1-3 are mapped out onto kernel thread 1 and thread 2 blocks, a lightweight context-switch can be made to thread 1 or 3 by the JVM. It is lightweight because it is an operation that occurs entirely in user space with no kernel intervention</i>"	threads
Sist. Operativos Final	Describe the 3 thread models	"<div>(Assuming a multi-processing core system)</div><div>- M:1 model: <u><b>Many to one.</b></u> <i>All the user-level threads of one process are mapped out onto a single kernel ""carrier"" thread. Offers the advantage of lightweight context switching within a single kernel thread, but presents the following disadvantage: In a multi-processing core system, the one processor can only use one core at a time<br><br></i> </div><div>- 1:1 model: <u><b>One to one.</b></u> <i>Each user level thread in the process' address space is mapped out onto one kernel thread. Offers the advantage of allowing one process to use multiple cores, but presents the disadvantage of significant overhead in the case that a context-switch is made from one user thread to another<br><br></i> </div><div>- M:N model: <u><b>Many to many.</b></u> <i>Threads from a - d are mapped out onto kernel thread 1, and threads e - h are mapped out onto kernel thread 2, for example. Combines the two prior models to balance their advantages and disadvantages.&nbsp;<br><br></i><img alt=""Three types of thread models. Popular operating systems [5], [22]-[24]... |  Download Scientific Diagram"" src=""https://www.researchgate.net/publication/346379550/figure/fig1/AS:974018400251914@1609235491930/Three-types-of-thread-models-Popular-operating-systems-5-22-24-adopt-the.ppm""><i><br></i></div>"	threads
Sist. Operativos Final	What does fork() return?	- 0 in the child process<br>- The child process' PID in the parent process<br>- -1 if fork() didn't execute successfully	processes
Sist. Operativos Final	How many semaphores are necessary in the producer-consumer problem? (critical section problem applied to buffers)	"<u><b>Three semaphores are necessary:</b></u><br><ul><li>Mutex: binary semaphore that ensures mutual exclusion and the atomic execution of the critical section within the two processes</li><li>Empty: counting semaphore which value references the amount of empty slots in the protected buffer. It is initialised to the N amount of slots available</li><li>Full: counting semaphore that indicates the amount of occupied or full slots within a buffer. It is initialised to 0</li></ul><br><img alt=""The Bounded Buffer Problem"" src=""maxresdefault.jpg""><br>"	process-synch
Sist. Operativos Final	Supposing a mutex semaphore is being implemented, where is the critical section of a process?	In the case of a mutex semaphore being implemented, the critical section is found between the mutex semaphore's decrement and increment	process-synch
Sist. Operativos Final	Minimum amount of semaphores needed to solve the producer-consumer problem?	The minimum amount of semaphores necessary in the producer-consumer problem if N = 1 is two: one that ensures mutual exclusion and one that keeps the producer from producing if the buffer is full and the consumer from consuming if the buffer is empty:<br><ul><li>mutex = 1;</li><li>empty = 1; //consumer only consumes in the case that empty is equal to 0 in this scenario</li></ul><br>The reason why only one semaphore representing if the buffer is empty or not is insufficient is because it makes no sense. A semaphore needs to be decremented in order for a process to enter it's critical section and it increments it once it is done to grant access to another process<br>	process-synch
Sist. Operativos Final	How many process states can the OS modify at a time, supposing a single processing core system?	In a single core system, the OS can only perform one process state change at a time<br>In a multiple core system however, it can change the state of various processes at a time	os-structure
Sist. Operativos Final	What OS component is in charge of making processes transition from the blocking state to the ready state after I/O event completion?	I/O event completion is signaled via an interrupt:<br>- The kernel executes the corresponding interrupt handler<br>- The interrupt handler changes the state of the process in question from waiting to ready and updates the queue<br>- The CPU either continues to execute the currently running process or switches to the newly admitted one if it is of higher priority *and* the system uses priority-based preemptive scheduling	processes
Sist. Operativos Final	Describe what a deadlock might look like in code	"<img src=""paste-d180270c6cb547354897edaecb530c6f8e1ca947.jpg"">"	deadlocks
Sist. Operativos Final	What does 'no preemption' mean in the context of deadlocks?	It means a resource cannot be forcibly taken away from a process unless it voluntarily gives it up (i.e. usually, when it has finished using it)	deadlocks
Sist. Operativos Final	What must occur for a deadlock to be prevented? What's one solution to the problem?	"<i><b>To solve a deadlock one of the four conditions</b></i> (<b><u>hold and wait</u></b>, a process holds a resource as it waits for another one; <b><u>no preemption</u></b>, process cannot be deallocated resources against it's 'own will'; <b><u>circular wait</u></b>, a process is holding a resource another process is requesting while holding a resource that the preceding process is requesting; <b><u>mutual exclusion</u></b>, processes' critical sections must be executed atomically) <i><b>needs to never occur.</b></i><br><br>One option is to remove circular waiting by ensuring processes access resources in an ascending order:
This is achieved by assigning numbers to resources in such a way that processes acquire them in a sequential and predictable number- a process holding a resource will not request one of smaller order"	deadlocks
Sist. Operativos Final	Explain the banker's algorithm in deadlocks	"Deadlocks can also be avoided (asides from preventing circular waiting through sequenced and predictable resource assignment) using the banker's algorithm, where using OS-maintained data structures resource allocation can be simulated by modifying the values in said data structures without *actually* granting any resources.<br><br>It follows these steps:<br><br>1. Checks that the request is valid (request &lt;= need &amp;&amp; request &lt;= available)
2. Simulates resource allocation<br>3. Does the same for all the other processes in the associated data structures; the OS tries to find an order in which all the processes can execute to completion successfully<br>4. If found, the resources are allocated. Otherwise, they are refused"	deadlocks
Sist. Operativos Final	Abstract data structures kept for the implementation of the banker's algorithm	The data structures kept to implement the banker's algorithm are:<br>- Available vector: how many resources are available at a given time<br>- Allocated matrix: how many resources are allocated to what processes at a given time<br>- Maximum matrix: each process' declared maximum of each resource<br>- Need: maximum - allocated	deadlocks
Sist. Operativos Final	Supposing a process writes to a private copy on a file, when/how do other processes accessing said file see the associated modifications?	"- Linux maintains an in-RAM cache of recently used file data<br>- Process writed to a file -&gt; data is copied into the cache (this is done to prevent the latency associated with accessing secondary memory/the disk)<br>- A reading process reads from the file<br>- Kernel checks the file cache first<br>- Reading process is able to see the updated file<br><br>Eventually, the OS will ""flush"" (empty the cache buffer and commit any ""dirty"" pages to secondary memory) the files to commit them to disk"	os-structure
Sist. Operativos Final	"How does the OS ""predict"" the length of the next CPU burst?"	"It ""predicts"" the length of the next CPU burst by performing calculations using as operands the last prediction it made and the *actual* duration of the last CPU burst<br><br>It allows for continuous adjustements. The data computed is stored in the corresponding process' PCB as scheduling information"	scheduling
Sist. Operativos Final	Briefly exemplify in C the differences between heap, data and stack segments- What is a memory leak?	"A memory leak occurs when dynamically allocated memory is no longer used by the program but is not released with free() (in C) or equivalent. Over time, memory leaks can exhaust heap memory, leading to allocation failures, degraded performance, or program crashes.<br><br><img src=""paste-56abcf1e8d0fa04b96ab50b0b865c3fa9e0ff469.jpg"">"	memory-admin
